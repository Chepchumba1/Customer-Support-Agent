# -*- coding: utf-8 -*-
"""customer support agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zQWHRId7qiFVNMR4HxtKD2KVxdLrZf7z
"""

!pip install langgraph langchain_groq langchain langchain-community faiss-cpu pypdf

#Importing libraries
import os
from typing import TypedDict, Dict
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_core.runnables.graph import MermaidDrawMethod
from langchain_groq import ChatGroq
from IPython.display import display, Image

#Creating the state class
class State(TypedDict):
    query: str
    category: str
    sentiment: str
    response: str

import os
os.environ["GROQ_API_KEY"] = "your_API_key"

# Check if the key is loaded successfully
if os.environ["GROQ_API_KEY"]:
    print("Groq API Key loaded successfully!")
else:
    print("Error: Groq API Key not found. Check your .env file.")

llm = ChatGroq(
    model="llama-3.1-8b-instant",
    temperature=0,
    api_key=os.environ["GROQ_API_KEY"]
)
#testing the model
result = llm.invoke("what is a laptop")
result.content

#Generic helper to run a prompt through the LLM
def run_prompt(template: str, query: str) -> str:
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm
    return chain.invoke({"query": query}).content

#Categorizing function
def categorize(state: State) -> State:
    template = (
        "Categorize the following customer query into one of these categories: "
        "Technical, Billing, General. Query: {query}"
    )
    return {"category": run_prompt(template, state["query"])}

#Sentiment analysis function
def analyze_sentiment(state: State) -> State:
    template = (
        "Analyze the sentiment of the following customer query. "
        "Respond with either 'Positive', 'Neutral', or 'Negative'. Query: {query}"
    )
    return {"sentiment": run_prompt(template, state["query"])}

#Handlers for different categories
def handle_technical(state: State) -> State:
    return {
        "response": run_prompt(
            "Provide a technical support response to the following query: {query}",
            state["query"]
        )
    }

def handle_billing(state: State) -> State:
    return {
        "response": run_prompt(
            "Provide a billing support response to the following query: {query}",
            state["query"]
        )
    }

def handle_general(state: State) -> State:
    return {
        "response": run_prompt(
            "Provide a general support response to the following query: {query}",
            state["query"]
        )
    }

#Escalate function
def escalate(state: State) -> State:
    return {
        "response": (
            "This query has been escalated to a human agent due to its negative sentiment."
        )
    }

# handling the routing function
def route_query(state: State) -> State:
    if state['sentiment'] == 'Negative':
        return "escalate"
    elif state['category'] == 'Technical':
        return "handle_technical"
    elif state['category'] == 'Billing':
        return "handle_billing"
    else:
        return "handle_general"

#crafting the workflow
workflow = StateGraph(State)
workflow.add_node("categorize", categorize)
workflow.add_node("analyze_sentiment", analyze_sentiment)
workflow.add_node("handle_technical", handle_technical)
workflow.add_node("handle_billing", handle_billing)
workflow.add_node("handle_general", handle_general)
workflow.add_node("escalate", escalate)

#adding edges
workflow.add_edge("categorize", "analyze_sentiment")
workflow.add_conditional_edges("analyze_sentiment",
route_query,{
    "handle_technical" : "handle_technical",
    "handle_billing": "handle_billing",
    "handle_general" : "handle_general",
    "escalate": "escalate"
})
workflow.add_edge("handle_technical", END)
workflow.add_edge("handle_billing", END)
workflow.add_edge("handle_general", END)
workflow.add_edge("escalate", END)

#aadding the entry point
workflow.set_entry_point("categorize")

#compile the workflow
app = workflow.compile()

#vizualise the graph
display(
    Image(
        app.get_graph().draw_mermaid_png(
            draw_method = MermaidDrawMethod.API
        )

))

#Function to run customer support
def run_customer_support(query: str) -> dict:
    result = app.invoke({"query": query})
    return {
        "category": result['category'],
        "sentiment": result['sentiment'],
        "response": result['response'],
    }

#testing the output
query = "What are your business hours?"
result = run_customer_support(query)
print(f"Query: {query}")
print(f"Category: {result['category']}")
print(f"Sentiment: {result['sentiment']}")
print(f"Response: {result['response']}")
print("\n")
